<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>M-realm Mirror with Face Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body { background: black; color: white; font-family: Arial, sans-serif; text-align: center; padding: 1em; }
    h2 { color: #00ffff; }
    video, canvas { border: 3px solid #00ffff; border-radius: 10px; margin-bottom: 1em; }
    textarea {
      width: 80%%; height: 100px; font-size: 1em;
      border-radius: 10px; padding: 1em; margin: 1em auto; border: none;
    }
    button {
      background: #00ffff; color: black; padding: 10px 20px;
      border: none; border-radius: 10px; cursor: pointer; font-size: 1em;
    }
    #results, #suggestion { margin-top: 1em; color: #00ffaa; font-size: 1.1em; }
  </style>
</head>
<body>
  <h2>M-realm Mirror: Face + Voice + Text</h2>
  <p>Triple emotion detection: facial expression, voice tone, and written word.</p>

  <video id="video" width="320" height="240" autoplay muted></video>
  <canvas id="overlay" width="320" height="240" style="position:absolute; left:50%%; transform:translateX(-50%%);"></canvas><br>
  <button onclick="startMirror()">Start Mirror</button>

  <textarea id="inputText" placeholder="Write how you feel..."></textarea><br>
  <button onclick="analyze()">Analyze All</button>

  <div id="results"></div>
  <div id="suggestion"></div>

  <script>
    let video = document.getElementById("video");
    let overlay = document.getElementById("overlay");
    let ctx = overlay.getContext("2d");
    let audioContext, analyser, dataArray, rms = 0, faceEmotion = "neutral";

    const emotionDict = {
      distress: ["hurt", "empty", "lost", "pain", "hopeless", "worthless"],
      joy: ["happy", "excited", "grateful", "cheerful", "blessed"],
      sadness: ["sad", "lonely", "regret", "cry", "depressed"],
      bond: ["love", "connection", "soulmate", "heartfelt"],
      anxiety: ["anxious", "panic", "nervous", "fear", "stress"],
      anger: ["angry", "mad", "furious", "rage"]
    };

    const suggestions = {
      joy: "🌟 Radiate joy. Share a smile today.",
      sadness: "🌧️ Slow down. Let emotions pass.",
      distress: "🆘 You're not alone. Breathe deeply.",
      anxiety: "🌬️ Focus on the now. One breath at a time.",
      anger: "🔥 Use movement or music to release.",
      bond: "💞 Express appreciation to someone close.",
      neutral: "🕊️ Ground yourself. This moment is yours."
    };

    async function startMirror() {
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/models"),
        faceapi.nets.faceExpressionNet.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/models")
      ]);

      navigator.mediaDevices.getUserMedia({ video: true, audio: true })
        .then(stream => {
          video.srcObject = stream;

          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          analyser = audioContext.createAnalyser();
          const mic = audioContext.createMediaStreamSource(stream);
          mic.connect(analyser);
          analyser.fftSize = 2048;
          dataArray = new Uint8Array(analyser.fftSize);
          monitorVolume();

          setInterval(async () => {
            const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
            ctx.clearRect(0, 0, overlay.width, overlay.height);
            if (detections) {
              faceapi.draw.drawDetections(overlay, detections);
              let expr = detections.expressions;
              let max = Object.keys(expr).reduce((a, b) => expr[a] > expr[b] ? a : b);
              faceEmotion = max;
            }
          }, 1000);
        })
        .catch(() => alert("Camera/Mic permission needed."));
    }

    function monitorVolume() {
      analyser.getByteTimeDomainData(dataArray);
      let sum = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const deviation = dataArray[i] - 128;
        sum += deviation * deviation;
      }
      rms = Math.sqrt(sum / dataArray.length);
      requestAnimationFrame(monitorVolume);
    }

    function getTextEmotion(text) {
      text = text.toLowerCase();
      for (let [emotion, words] of Object.entries(emotionDict)) {
        if (words.some(word => text.includes(word))) {
          return emotion;
        }
      }
      return "neutral";
    }

    function getVoiceEmotion(rms) {
      if (rms > 30) return "anger";
      if (rms > 15) return "anxiety";
      if (rms > 5) return "joy";
      if (rms > 2) return "sadness";
      return "neutral";
    }

    function analyze() {
      const text = document.getElementById("inputText").value;
      const textEmotion = getTextEmotion(text);
      const voiceEmotion = getVoiceEmotion(rms);
      const faceFinal = faceEmotion === "happy" ? "joy" :
                        faceEmotion === "angry" ? "anger" :
                        faceEmotion === "sad" ? "sadness" :
                        faceEmotion;

      let result = `🧠 Text: ${textEmotion.toUpperCase()} | 🎤 Voice: ${voiceEmotion.toUpperCase()} | 🧍‍♂️ Face: ${faceFinal.toUpperCase()}`;
      let final = [textEmotion, voiceEmotion, faceFinal].filter(e => e !== "neutral")[0] || "neutral";
      document.getElementById("results").innerText = result;
      document.getElementById("suggestion").innerText = `💡 Suggestion: ${suggestions[final]}`;

      // Memory logging
      const history = JSON.parse(localStorage.getItem("emotionHistory") || "[]");
      history.push({ timestamp: new Date().toISOString(), emotion: final, source: "mirror" });
      localStorage.setItem("emotionHistory", JSON.stringify(history.slice(-10)));
    }
  </script>
</body>
</html>
